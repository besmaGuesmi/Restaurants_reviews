{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Restaurant_Reviews.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPTl2QQAjp46aPVvR71pmDE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAut2C11Rg6P",
        "colab_type": "code",
        "outputId": "c2e88277-e61b-4e11-fb70-cc1d61428cb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# Step 1: Import dataset\n",
        "# Importing Libraries \n",
        "import numpy as np   \n",
        "import pandas as pd  \n",
        "  \n",
        "# Import dataset \n",
        "dataset = pd.read_csv('/content/Restaurant_Reviews.tsv', delimiter = '\\t')  \n",
        "dataset.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>Liked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Review  Liked\n",
              "0                           Wow... Loved this place.      1\n",
              "1                                 Crust is not good.      0\n",
              "2          Not tasty and the texture was just nasty.      0\n",
              "3  Stopped by during the late May bank holiday of...      1\n",
              "4  The selection on the menu was great and so wer...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMG9K971TKoT",
        "colab_type": "code",
        "outputId": "8be99b3d-373b-4a36-c7f5-52a513e944f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#Step 2: Text Cleaning or Preprocessing\n",
        "#2.1:remove punctuations, numbers...\n",
        "#2.2:Stemming: Take roots of the word\n",
        "#2.3:convert each word into its lower case\n",
        "\n",
        "# library to clean data \n",
        "import re  \n",
        "  \n",
        "# Natural Language Tool Kit \n",
        "import nltk  \n",
        "  \n",
        "nltk.download('stopwords') \n",
        "  \n",
        "# to remove stopword \n",
        "from nltk.corpus import stopwords \n",
        "  \n",
        "# for Stemming propose  \n",
        "from nltk.stem.porter import PorterStemmer \n",
        "  \n",
        "# Initialize empty array \n",
        "# to append clean text  \n",
        "corpus = []  \n",
        "  \n",
        "# 1000 (reviews) rows to clean \n",
        "for i in range(0, 1000):  \n",
        "      \n",
        "    # column : \"Review\", row ith \n",
        "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])  \n",
        "      \n",
        "    # convert all cases to lower cases \n",
        "    review = review.lower()  \n",
        "      \n",
        "    # split to array(default delimiter is \" \") \n",
        "    review = review.split()  \n",
        "      \n",
        "    # creating PorterStemmer object to \n",
        "    # take main stem of each word \n",
        "    ps = PorterStemmer()  \n",
        "      \n",
        "    # loop for stemming each word \n",
        "    # in string array at ith row     \n",
        "    review = [ps.stem(word) for word in review \n",
        "                if not word in set(stopwords.words('english'))]  \n",
        "                  \n",
        "    # rejoin all string array elements \n",
        "    # to create back into a string \n",
        "    review = ' '.join(review)   \n",
        "      \n",
        "    # append each string to create \n",
        "    # array of clean text  \n",
        "    corpus.append(review)  "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqFuTHBBjKL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step 3: Tokenization, involves splitting sentences and words from the body of the text.\n",
        "#step4 : Making the bag of words via sparse matrix\n",
        "#creating the bag of words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# To extract max 1500 feature. \n",
        "# \"max_features\" is attribute to \n",
        "# experiment with to get better results \n",
        "cv = CountVectorizer(max_features = 1500)  \n",
        "  \n",
        "# X contains corpus (dependent variable) \n",
        "X = cv.fit_transform(corpus).toarray()  \n",
        "  \n",
        "# y contains answers if review is positive or negative \n",
        "y = dataset.iloc[:, 1].values "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hbCcXd2zihe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step 5 : Splitting Corpus into Training and Test set\n",
        "\n",
        "# Splitting the dataset into \n",
        "# the Training set and Test set \n",
        "from sklearn.model_selection import train_test_split  \n",
        "# experiment with \"test_size\" \n",
        "# to get better results \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQxOIJ2S1WJ7",
        "colab_type": "code",
        "outputId": "e7d53962-246b-46ff-b2a8-819a14c19cc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "#step6: fitting a predictive model\n",
        "# Fitting Random Forest Classification \n",
        "# to the Training set \n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "  \n",
        "# n_estimators can be said as number of \n",
        "# trees, experiment with n_estimators \n",
        "# to get better results  \n",
        "model = RandomForestClassifier(n_estimators = 501, \n",
        "                            criterion = 'entropy') \n",
        "                              \n",
        "model.fit(X_train, y_train)   "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='entropy', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=501,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPdAZOO39HRX",
        "colab_type": "code",
        "outputId": "520f50bf-7cb8-4868-9efb-568dfa2d8afe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "#predicting final resultats\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "       1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
              "       0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
              "       0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
              "       1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
              "       1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
              "       0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "       0, 0, 1, 0, 0, 1, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    }
  ]
}